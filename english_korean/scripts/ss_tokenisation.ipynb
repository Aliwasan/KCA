{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Sous-tokenisation avec SentencePiece ##"],"metadata":{"id":"e03lhagRJ1CK"}},{"cell_type":"code","source":["!pip install sentencepiece"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aJCfI1kRJzSv","outputId":"68846d6a-7df2-4883-b650-a014e5e18153","executionInfo":{"status":"ok","timestamp":1704639302709,"user_tz":-60,"elapsed":6722,"user":{"displayName":"Camille CLAVIER","userId":"08908395152911270822"}}},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (0.1.99)\n"]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# On se sert des corpus généraux pour entraîner le modèle de sous-tokénisation\n","# Seront tokénisés avec ces modèles les corpus généraux et spécifiques\n","en_path = '/content/drive/My Drive/Colab Notebooks/gen_token_en.txt'\n","ko_path = '/content/drive/My Drive/Colab Notebooks/gen_token_ko.txt'\n","en_ss_path = '/content/drive/My Drive/Colab Notebooks/spe_token_en.txt'\n","ko_ss_path = '/content/drive/My Drive/Colab Notebooks/spe_token_ko.txt'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e3uWhcOKItEz","executionInfo":{"status":"ok","timestamp":1704640411176,"user_tz":-60,"elapsed":1839,"user":{"displayName":"Camille CLAVIER","userId":"08908395152911270822"}},"outputId":"7b986d0c-1b42-458f-c0b1-2465805ab23c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["import sentencepiece as spm\n","\n","# Penser à changer de variable en fonction de la langue sous-tokenisé\n","\n","#Entraînement du modèle\n","model = spm.SentencePieceTrainer.train(\n","    input= ko_path,\n","    model_prefix='gen_token_ko',\n","    vocab_size=1345, # 226 pour l'anglais\n","    character_coverage=0.9995, #1.0 pour l'anglais\n","    model_type='unigram'\n",")\n","\n","# Charger le modèle SentencePiece\n","sp = spm.SentencePieceProcessor()\n","sp.load('gen_token_ko.model')"],"metadata":{"id":"i05JPiNMKLPw","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1704640623876,"user_tz":-60,"elapsed":271,"user":{"displayName":"Camille CLAVIER","userId":"08908395152911270822"}},"outputId":"b82fa38c-b146-4b81-f136-81c943d1ec50"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":23}]},{"cell_type":"code","source":["with open(ko_ss_path, 'r', encoding='utf-8') as file:\n","    corpus = file.readlines()\n","\n","output_file_path = '/content/drive/My Drive/Colab Notebooks/spe_ss_token_ko.txt'\n","\n","with open(output_file_path, 'w', encoding='utf-8') as output_file:\n","    for sentence in corpus:\n","        # sous-tokenisation\n","        tokens = sp.encode(sentence, out_type=str)\n","\n","        # Ecriture du résultat\n","        for token in tokens:\n","            output_file.write(token + '\\n')\n","\n","print(f\"Le corpus sous-tokenisé a été enregistré dans {output_file_path}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"us5gj2mVZYIm","executionInfo":{"status":"ok","timestamp":1704640630517,"user_tz":-60,"elapsed":771,"user":{"displayName":"Camille CLAVIER","userId":"08908395152911270822"}},"outputId":"58f80adb-ad57-4f12-c111-48a5da87a573"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Le corpus sous-tokenisé a été enregistré dans /content/drive/My Drive/Colab Notebooks/spe_ss_token_ko.txt\n"]}]}]}